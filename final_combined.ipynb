{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:110%\">\n",
    "\n",
    "In this notebook, we apply the following methods:  \n",
    "1. Optimal Rank-\\(k\\) SVD, \n",
    "2. Embedding Dot-Product Model\n",
    "3. Iterative SVD, \n",
    "4. SVD\\++, \n",
    "5. NeuMF\n",
    "6. GraphNeuMF, \n",
    "7. DMF \n",
    "8. Ensemble strategies (simple average, weighted average, top-\\(k\\) weighted, deep stacking).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from torch_geometric.nn import LGConv\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = \"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.1)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def read_data_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns matrix view of the training data, where columns are scientists (sid) and\n",
    "    rows are papers (pid).\"\"\"\n",
    "\n",
    "    return df.pivot(index=\"sid\", columns=\"pid\", values=\"rating\").values\n",
    "\n",
    "\n",
    "def read_wishlist_df() -> pd.DataFrame:\n",
    "    \"\"\"Reads in the wishlist data (train_tbr.csv).\"\"\"\n",
    "    \n",
    "    tbr_df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))\n",
    "    # Ensure sid and pid are integers\n",
    "    tbr_df[\"sid\"] = tbr_df[\"sid\"].astype(int)\n",
    "    tbr_df[\"pid\"] = tbr_df[\"pid\"].astype(int)\n",
    "    return tbr_df\n",
    "\n",
    "\n",
    "def read_wishlist_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns matrix view of the wishlist data.\n",
    "    Rows are scientists (sid), columns are papers (pid).\n",
    "    Values are 1 if the paper is on the wishlist, NaN otherwise.\"\"\"\n",
    "    \n",
    "    # Add a temporary column with value 1 for pivoting\n",
    "    df_copy = df.copy()\n",
    "    df_copy['wishlisted'] = 1\n",
    "\n",
    "\n",
    "    all_sids = pd.Index(range(10000))\n",
    "    wishlist_matrix = df_copy.pivot(index=\"sid\", columns=\"pid\", values=\"wishlisted\")\n",
    "    wishlist_matrix = wishlist_matrix.reindex(all_sids)\n",
    "\n",
    "    return wishlist_matrix.values\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "    \n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(mat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Fills in missing values of the matrix\"\"\"\n",
    "    return np.nan_to_num(mat, nan=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings_df, valid_ratings_df = read_data_df()\n",
    "train_wishlist_df = read_wishlist_df()\n",
    "\n",
    "train_mat = read_data_matrix(train_ratings_df)\n",
    "train_mat = impute_values(train_mat)\n",
    "\n",
    "train_wishlist_mat = read_wishlist_matrix(train_wishlist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Low-Rank SVD Approximation\n",
    "\n",
    "Next, we take a linear algebra view of the researcher–paper matrix \\(M\\) by computing its SVD:\n",
    "\n",
    "$$\n",
    "M = U \\,\\Sigma\\,V^T\n",
    "$$\n",
    "\n",
    "We then truncate to the top \\(k\\) singular components:\n",
    "\n",
    "$$\n",
    "U_k = U[:, :k], \\quad \\Sigma_k = \\Sigma_{1:k}, \\quad V_k^T = V^T[:k, :]\n",
    "$$\n",
    "\n",
    "to form the low-rank reconstruction\n",
    "\n",
    "$$\n",
    "M_k = U_k \\,\\Sigma_k\\,V_k^T\n",
    "$$\n",
    "\n",
    "Finally, we score any researcher–paper pair \\((r,p)\\) by indexing into \\(M_k\\):\n",
    "\n",
    "$$\n",
    "\\hat r_{rp} = (M_k)_{r,p}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "singular_values = np.linalg.svd(train_mat, compute_uv=False, hermitian=False)\n",
    "plt.plot(singular_values)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Singular value spectrum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the researcher–paper matrix to its top-k singular components to capture the most important interaction patterns. Then we provide a simple lookup function that fetches the predicted score for any researcher–paper pair from this compressed representation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_rank_k_approximation(m: np.ndarray, k: int):\n",
    "    \"\"\"Returns the optimal rank-k reconstruction matrix, using SVD.\"\"\"\n",
    "    \n",
    "    assert 0 < k <= np.min(m.shape), f\"The rank must be in [0, min(m, n)]\"\n",
    "    \n",
    "    U, S, Vh = np.linalg.svd(m, full_matrices=False)\n",
    "    \n",
    "    U_k = U[:, :k]\n",
    "    S_k = S[:k]\n",
    "    Vh_k = Vh[:k]\n",
    "    \n",
    "    return np.dot(U_k * S_k, Vh_k)\n",
    "\n",
    "\n",
    "def matrix_pred_fn(train_recon: np.ndarray, sids: np.ndarray, pids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        train_recon: (M, N) matrix with predicted values for every (sid, pid) pair.\n",
    "        sids: (D,) vector with integer scientist IDs.\n",
    "        pids: (D,) vector with integer paper IDs.\n",
    "        \n",
    "    Outputs: (D,) vector with predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    return train_recon[sids, pids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute a rank-2 low-rank approximation of the training matrix via SVD by calling `opt_rank_k_approximation(train_mat, k=2)`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recon = opt_rank_k_approximation(train_mat, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.186\n"
     ]
    }
   ],
   "source": [
    "pred_fn = lambda sids, pids: matrix_pred_fn(train_recon, sids, pids)\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_score = evaluate(valid_ratings_df, pred_fn)\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Learned embeddings\n",
    "\n",
    "Next, we will take a machine learning view of the problem. To each scientist and paper, we assign a $d$-dimensional embedding and we predict the rating that the scientist gives the paper to be their dot product. More formally, let $\\vec{s}_i$ be a scientist embedding and $\\vec{p}_j$ be a paper embedding. Then, we make the following rating prediction for this pair: $$\\tilde{r}_{ij} = \\langle \\vec{s}_i, \\vec{p}_j \\rangle.$$ We view these embeddings as our learnable parameters and train them as we would any other model using the squared error loss function: $$\\ell(\\theta) = \\frac{1}{2} |\\langle \\vec{s}_i, \\vec{p}_j \\rangle - r_{ij}|^2,$$ where $\\theta = \\{ \\vec{s}_i \\}_{i=1}^n \\cup \\{ \\vec{p}_j \\}_{j=1}^m$. The following is an implementation of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDotProductModel(nn.Module):\n",
    "    def __init__(self, num_scientists: int, num_papers: int, dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign to each scientist and paper an embedding\n",
    "        self.scientist_emb = nn.Embedding(num_scientists, dim)\n",
    "        self.paper_emb = nn.Embedding(num_papers, dim)\n",
    "        \n",
    "    def forward(self, sid: torch.Tensor, pid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            sid: [B,], int\n",
    "            pid: [B,], int\n",
    "        \n",
    "        Outputs: [B,], float\n",
    "        \"\"\"\n",
    "\n",
    "        # Per-pair dot product\n",
    "        return torch.sum(self.scientist_emb(sid) * self.paper_emb(pid), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (10k scientists, 1k papers, 32-dimensional embeddings) and optimizer\n",
    "model = EmbeddingDotProductModel(10_000, 1_000, 64).to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the Pandas DataFrames into PyTorch `TensorDataset`s containing researcher IDs, paper IDs, and ratings for easy batching. \n",
    "\n",
    "We then create `DataLoader`s with batch size 64—shuffled for training and sequential for validation—to feed data into our models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df: pd.DataFrame):\n",
    "    sids = torch.tensor(df[\"sid\"].values, dtype=torch.long)\n",
    "    pids = torch.tensor(df[\"pid\"].values, dtype=torch.long)\n",
    "    ratings = torch.tensor(df[\"rating\"].values, dtype=torch.float32)\n",
    "    return torch.utils.data.TensorDataset(sids, pids, ratings)\n",
    "\n",
    "train_dataset = get_dataset(train_ratings_df)\n",
    "valid_dataset = get_dataset(valid_ratings_df)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for $5$ epochs over researcher–paper batches, minimizing MSE and updating parameters via backpropagation, but the training loss isn’t improving substantially. \n",
    "\n",
    "We’ll need to explore other more advanced techniques to boost performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] Train Loss: 53.4843\n",
      "[Epoch 2/5] Train Loss: 19.9365\n",
      "[Epoch 3/5] Train Loss: 3.3771\n",
      "[Epoch 4/5] Train Loss: 1.2094\n",
      "[Epoch 5/5] Train Loss: 0.9402\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, total_count = 0.0, 0\n",
    "    for sid, pid, ratings in train_loader:\n",
    "        sid = sid.to(device)\n",
    "        pid = pid.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        preds = model(sid, pid)\n",
    "        loss = F.mse_loss(preds, ratings)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        total_loss += loss.item() * len(sid)\n",
    "        total_count += len(sid)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Train Loss: {total_loss / total_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.042\n"
     ]
    }
   ],
   "source": [
    "pred_fn = lambda sids, pids: model(torch.from_numpy(sids).to(device), torch.from_numpy(pids).to(device))\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_ratings_df, pred_fn)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Iterative SVD Imputation\n",
    "\n",
    "We start with the incomplete researcher–paper matrix \\(M\\), fill missing entries with initial estimates to get \\(M^{(0)}\\), and then repeat:\n",
    "\n",
    "Compute truncated SVD on the current matrix:\n",
    "   $$\n",
    "   M^{(t-1)} = U\\,\\Sigma\\,V^T,\n",
    "   \\qquad\n",
    "   M_k^{(t)} = U_k\\,\\Sigma_k\\,V_k^T\n",
    "   $$\n",
    "\n",
    "Update only the missing entries:\n",
    "   $$\n",
    "   M^{(t)}_{ij} =\n",
    "   \\begin{cases}\n",
    "   M_{ij}, & \\text{if }(i,j)\\text{ observed},\\\\\n",
    "   \\bigl(M_k^{(t)}\\bigr)_{ij}, & \\text{if }(i,j)\\text{ missing}.\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "Measure change on the missing entries:\n",
    "   $$\n",
    "   \\Delta^{(t)} = \\bigl\\|\\,\\bigl(M_k^{(t)}\\bigr)_{\\text{missing}} \\;-\\; M^{(t-1)}_{\\text{missing}} \\bigr\\|_2\n",
    "   $$\n",
    "\n",
    "We stop when \\(\\Delta^{(t)}\\) falls below the tolerance, and then use the final matrix \\(M^{(*)}\\) to score researcher–paper pairs by simple indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_svd(df, k=2, max_iterations=10, tolerance=0.001):\n",
    "    mat = read_data_matrix(df)\n",
    "    missing_mask = np.isnan(mat)\n",
    "    imputed_mat = impute_values(mat)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        previous_mat = imputed_mat.copy()\n",
    "        reconstructed_mat = opt_rank_k_approximation(imputed_mat, k)\n",
    "\n",
    "        imputed_mat = mat.copy()\n",
    "        imputed_mat[missing_mask] = reconstructed_mat[missing_mask]\n",
    "\n",
    "        diff = np.linalg.norm(reconstructed_mat[missing_mask] - previous_mat[missing_mask])\n",
    "        print(f\"Iteration {iteration + 1}, Change: {diff:.6f}\")\n",
    "\n",
    "        if diff < tolerance:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    return imputed_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose $k=2$ to capture the two most significant latent factors, set `max_iterations=200` for sufficient refinement, and use a tolerance of $10^{-4}$ to ensure stable convergence.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Change: 358.604161\n",
      "Iteration 2, Change: 281.641703\n",
      "Iteration 3, Change: 229.566501\n",
      "Iteration 4, Change: 192.320822\n",
      "Iteration 5, Change: 164.490904\n",
      "Iteration 6, Change: 142.951880\n",
      "Iteration 7, Change: 125.801054\n",
      "Iteration 8, Change: 111.826364\n",
      "Iteration 9, Change: 100.224236\n",
      "Iteration 10, Change: 90.442575\n",
      "Iteration 11, Change: 82.089858\n",
      "Iteration 12, Change: 74.880570\n",
      "Iteration 13, Change: 68.601350\n",
      "Iteration 14, Change: 63.089309\n",
      "Iteration 15, Change: 58.217755\n",
      "Iteration 16, Change: 53.886528\n",
      "Iteration 17, Change: 50.015309\n",
      "Iteration 18, Change: 46.538876\n",
      "Iteration 19, Change: 43.403681\n",
      "Iteration 20, Change: 40.565323\n",
      "Iteration 21, Change: 37.986667\n",
      "Iteration 22, Change: 35.636408\n",
      "Iteration 23, Change: 33.487973\n",
      "Iteration 24, Change: 31.518657\n",
      "Iteration 25, Change: 29.708947\n",
      "Iteration 26, Change: 28.041987\n",
      "Iteration 27, Change: 26.503140\n",
      "Iteration 28, Change: 25.079641\n",
      "Iteration 29, Change: 23.760309\n",
      "Iteration 30, Change: 22.535311\n",
      "Iteration 31, Change: 21.395969\n",
      "Iteration 32, Change: 20.334598\n",
      "Iteration 33, Change: 19.344366\n",
      "Iteration 34, Change: 18.419185\n",
      "Iteration 35, Change: 17.553613\n",
      "Iteration 36, Change: 16.742769\n",
      "Iteration 37, Change: 15.982267\n",
      "Iteration 38, Change: 15.268155\n",
      "Iteration 39, Change: 14.596862\n",
      "Iteration 40, Change: 13.965154\n",
      "Iteration 41, Change: 13.370100\n",
      "Iteration 42, Change: 12.809030\n",
      "Iteration 43, Change: 12.279514\n",
      "Iteration 44, Change: 11.779334\n",
      "Iteration 45, Change: 11.306461\n",
      "Iteration 46, Change: 10.859035\n",
      "Iteration 47, Change: 10.435351\n",
      "Iteration 48, Change: 10.033840\n",
      "Iteration 49, Change: 9.653060\n",
      "Iteration 50, Change: 9.291679\n",
      "Iteration 51, Change: 8.948469\n",
      "Iteration 52, Change: 8.622295\n",
      "Iteration 53, Change: 8.312104\n",
      "Iteration 54, Change: 8.016924\n",
      "Iteration 55, Change: 7.735850\n",
      "Iteration 56, Change: 7.468043\n",
      "Iteration 57, Change: 7.212724\n",
      "Iteration 58, Change: 6.969164\n",
      "Iteration 59, Change: 6.736689\n",
      "Iteration 60, Change: 6.514667\n",
      "Iteration 61, Change: 6.302509\n",
      "Iteration 62, Change: 6.099666\n",
      "Iteration 63, Change: 5.905624\n",
      "Iteration 64, Change: 5.719903\n",
      "Iteration 65, Change: 5.542051\n",
      "Iteration 66, Change: 5.371649\n",
      "Iteration 67, Change: 5.208300\n",
      "Iteration 68, Change: 5.051635\n",
      "Iteration 69, Change: 4.901305\n",
      "Iteration 70, Change: 4.756984\n",
      "Iteration 71, Change: 4.618366\n",
      "Iteration 72, Change: 4.485162\n",
      "Iteration 73, Change: 4.357100\n",
      "Iteration 74, Change: 4.233927\n",
      "Iteration 75, Change: 4.115401\n",
      "Iteration 76, Change: 4.001296\n",
      "Iteration 77, Change: 3.891399\n",
      "Iteration 78, Change: 3.785510\n",
      "Iteration 79, Change: 3.683437\n",
      "Iteration 80, Change: 3.585004\n",
      "Iteration 81, Change: 3.490040\n",
      "Iteration 82, Change: 3.398386\n",
      "Iteration 83, Change: 3.309892\n",
      "Iteration 84, Change: 3.224415\n",
      "Iteration 85, Change: 3.141821\n",
      "Iteration 86, Change: 3.061982\n",
      "Iteration 87, Change: 2.984777\n",
      "Iteration 88, Change: 2.910093\n",
      "Iteration 89, Change: 2.837822\n",
      "Iteration 90, Change: 2.767861\n",
      "Iteration 91, Change: 2.700113\n",
      "Iteration 92, Change: 2.634487\n",
      "Iteration 93, Change: 2.570894\n",
      "Iteration 94, Change: 2.509252\n",
      "Iteration 95, Change: 2.449482\n",
      "Iteration 96, Change: 2.391511\n",
      "Iteration 97, Change: 2.335266\n",
      "Iteration 98, Change: 2.280680\n",
      "Iteration 99, Change: 2.227689\n",
      "Iteration 100, Change: 2.176233\n",
      "Iteration 101, Change: 2.126253\n",
      "Iteration 102, Change: 2.077693\n",
      "Iteration 103, Change: 2.030502\n",
      "Iteration 104, Change: 1.984628\n",
      "Iteration 105, Change: 1.940024\n",
      "Iteration 106, Change: 1.896645\n",
      "Iteration 107, Change: 1.854447\n",
      "Iteration 108, Change: 1.813388\n",
      "Iteration 109, Change: 1.773429\n",
      "Iteration 110, Change: 1.734531\n",
      "Iteration 111, Change: 1.696659\n",
      "Iteration 112, Change: 1.659777\n",
      "Iteration 113, Change: 1.623854\n",
      "Iteration 114, Change: 1.588856\n",
      "Iteration 115, Change: 1.554754\n",
      "Iteration 116, Change: 1.521518\n",
      "Iteration 117, Change: 1.489121\n",
      "Iteration 118, Change: 1.457536\n",
      "Iteration 119, Change: 1.426737\n",
      "Iteration 120, Change: 1.396700\n",
      "Iteration 121, Change: 1.367401\n",
      "Iteration 122, Change: 1.338817\n",
      "Iteration 123, Change: 1.310928\n",
      "Iteration 124, Change: 1.283711\n",
      "Iteration 125, Change: 1.257146\n",
      "Iteration 126, Change: 1.231216\n",
      "Iteration 127, Change: 1.205900\n",
      "Iteration 128, Change: 1.181181\n",
      "Iteration 129, Change: 1.157041\n",
      "Iteration 130, Change: 1.133465\n",
      "Iteration 131, Change: 1.110436\n",
      "Iteration 132, Change: 1.087939\n",
      "Iteration 133, Change: 1.065959\n",
      "Iteration 134, Change: 1.044481\n",
      "Iteration 135, Change: 1.023492\n",
      "Iteration 136, Change: 1.002978\n",
      "Iteration 137, Change: 0.982927\n",
      "Iteration 138, Change: 0.963326\n",
      "Iteration 139, Change: 0.944163\n",
      "Iteration 140, Change: 0.925426\n",
      "Iteration 141, Change: 0.907105\n",
      "Iteration 142, Change: 0.889188\n",
      "Iteration 143, Change: 0.871664\n",
      "Iteration 144, Change: 0.854525\n",
      "Iteration 145, Change: 0.837759\n",
      "Iteration 146, Change: 0.821358\n",
      "Iteration 147, Change: 0.805312\n",
      "Iteration 148, Change: 0.789612\n",
      "Iteration 149, Change: 0.774249\n",
      "Iteration 150, Change: 0.759215\n",
      "Iteration 151, Change: 0.744503\n",
      "Iteration 152, Change: 0.730103\n",
      "Iteration 153, Change: 0.716009\n",
      "Iteration 154, Change: 0.702213\n",
      "Iteration 155, Change: 0.688708\n",
      "Iteration 156, Change: 0.675486\n",
      "Iteration 157, Change: 0.662541\n",
      "Iteration 158, Change: 0.649867\n",
      "Iteration 159, Change: 0.637457\n",
      "Iteration 160, Change: 0.625304\n",
      "Iteration 161, Change: 0.613403\n",
      "Iteration 162, Change: 0.601748\n",
      "Iteration 163, Change: 0.590333\n",
      "Iteration 164, Change: 0.579152\n",
      "Iteration 165, Change: 0.568201\n",
      "Iteration 166, Change: 0.557473\n",
      "Iteration 167, Change: 0.546964\n",
      "Iteration 168, Change: 0.536669\n",
      "Iteration 169, Change: 0.526583\n",
      "Iteration 170, Change: 0.516701\n",
      "Iteration 171, Change: 0.507018\n",
      "Iteration 172, Change: 0.497531\n",
      "Iteration 173, Change: 0.488234\n",
      "Iteration 174, Change: 0.479124\n",
      "Iteration 175, Change: 0.470196\n",
      "Iteration 176, Change: 0.461447\n",
      "Iteration 177, Change: 0.452872\n",
      "Iteration 178, Change: 0.444467\n",
      "Iteration 179, Change: 0.436229\n",
      "Iteration 180, Change: 0.428155\n",
      "Iteration 181, Change: 0.420240\n",
      "Iteration 182, Change: 0.412482\n",
      "Iteration 183, Change: 0.404876\n",
      "Iteration 184, Change: 0.397420\n",
      "Iteration 185, Change: 0.390110\n",
      "Iteration 186, Change: 0.382943\n",
      "Iteration 187, Change: 0.375917\n",
      "Iteration 188, Change: 0.369027\n",
      "Iteration 189, Change: 0.362272\n",
      "Iteration 190, Change: 0.355648\n",
      "Iteration 191, Change: 0.349153\n",
      "Iteration 192, Change: 0.342784\n",
      "Iteration 193, Change: 0.336538\n",
      "Iteration 194, Change: 0.330413\n",
      "Iteration 195, Change: 0.324406\n",
      "Iteration 196, Change: 0.318515\n",
      "Iteration 197, Change: 0.312737\n",
      "Iteration 198, Change: 0.307070\n",
      "Iteration 199, Change: 0.301512\n",
      "Iteration 200, Change: 0.296060\n"
     ]
    }
   ],
   "source": [
    "final_mat = iterative_svd(train_ratings_df, k=2, max_iterations=200, tolerance=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.875\n"
     ]
    }
   ],
   "source": [
    "pred_iter_svd = lambda sids, pids: matrix_pred_fn(final_mat, sids, pids).flatten()\n",
    "\n",
    "val_score = evaluate(valid_ratings_df, pred_iter_svd)\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation performance is just enough to pass the simple baseline, so we will now explore advanced neural methods like NeuMF, DMF, or hybrid combinations of both to improve recommendation accuracy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: SVD++\n",
    "\n",
    "We extend basic matrix factorization by adding user/item biases and modeling implicit feedback. Each researcher $r$ and paper $p$ has embeddings $\\mathbf{p}_r,\\mathbf{q}_p\\in\\mathbb{R}^d$, biases $b_r,b_p\\in\\mathbb{R}$, and implicit-feedback embeddings $\\{y_j\\}$. We predict\n",
    "\n",
    "$$\n",
    "\\hat r_{rp} = \\mu + b_r + b_p + \\mathbf{q}_p^T\\Bigl(\\mathbf{p}_r + \\frac{1}{\\sqrt{|\\mathcal{N}(r)|}}\\sum_{j\\in\\mathcal{N}(r)} y_j\\Bigr),\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the global mean and $\\mathcal{N}(r)$ is the set of papers researcher $r$ has interacted with. We train all parameters by minimizing the squared error loss\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\frac12\\bigl(\\hat r_{rp} - r_{rp}\\bigr)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDPP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.implicit_emb = nn.Embedding(num_items + 1, emb_dim, padding_idx=num_items)  # Padding index = num_items\n",
    "        self.global_mean = nn.Parameter(torch.tensor(3.0), requires_grad=False)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, implicit_items):\n",
    "        p_u = self.user_emb(user_ids)\n",
    "        q_i = self.item_emb(item_ids)\n",
    "        b_u = self.user_bias(user_ids).squeeze()\n",
    "        b_i = self.item_bias(item_ids).squeeze()\n",
    "\n",
    "        mask = (implicit_items != self.implicit_emb.padding_idx).unsqueeze(-1)\n",
    "        y_j = self.implicit_emb(implicit_items) * mask\n",
    "        norm = torch.sqrt(mask.sum(dim=1).clamp(min=1).float())\n",
    "        implicit_sum = y_j.sum(dim=1) / norm\n",
    "\n",
    "        x_u = p_u + implicit_sum\n",
    "        dot = (x_u * q_i).sum(dim=1)\n",
    "\n",
    "        return self.global_mean + b_u + b_i + dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the SVD++ model for $10\\,000$ researchers and $1\\,000$ papers, each represented in a $16$-dimensional embedding space to capture diverse latent factors.\n",
    "\n",
    "The Adam optimizer is configured with a learning rate of $10^{-3}$ and an L2 weight decay of $10^{-4}$ to balance rapid learning with regularization.\n",
    "\n",
    "A ReduceLROnPlateau scheduler monitors the validation loss and, if it does not improve for one epoch, halves the learning rate to allow finer-grained updates and more stable convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svdpp = SVDPP(num_users=10_000, num_items=1_000, emb_dim=16).to(device)\n",
    "optim_svdpp = torch.optim.Adam(model_svdpp.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_svdpp, mode='min', factor=0.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a fixed-size implicit-feedback matrix where each of the 10 000 researchers is assigned a list of up to 50 papers they have interacted with.  \n",
    "\n",
    "Using a `defaultdict`, we first collect all paper IDs for each researcher. We then build a NumPy array of shape (10 000, 50), filling unused slots with a padding index equal to the number of items.  \n",
    "\n",
    "Finally, we convert this array into a PyTorch tensor on the target device so it can be fed directly into the SVD++ model as implicit feedback.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_implicit_item_matrix(df, num_users, num_items, max_len=50):\n",
    "    PAD_IDX = num_items\n",
    "    user_implicit = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        user_implicit[int(row['sid'])].append(int(row['pid']))\n",
    "    matrix = np.full((num_users, max_len), fill_value=PAD_IDX, dtype=int)\n",
    "    for user_id, items in user_implicit.items():\n",
    "        matrix[user_id, :min(len(items), max_len)] = items[:max_len]\n",
    "    return torch.tensor(matrix, dtype=torch.long)\n",
    "\n",
    "NUM_USERS = 10_000\n",
    "NUM_ITEMS = 1_000\n",
    "EMB_DIM = 16\n",
    "MAX_LEN = 50\n",
    "\n",
    "implicit_matrix = build_implicit_item_matrix(train_ratings_df, NUM_USERS, NUM_ITEMS, max_len=MAX_LEN).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 10 epochs, we alternate between training and validation.  \n",
    "\n",
    "During training, for each batch of researcher–paper pairs we mask out the current paper from the researcher’s implicit list, predict ratings with SVD++, compute MSE loss, and update parameters via Adam.  \n",
    "\n",
    "In validation, we again mask the held‐out paper, compute predictions without gradient updates, and aggregate the squared errors to report RMSE.  \n",
    "\n",
    "A ReduceLROnPlateau scheduler adjusts the learning rate based on the validation loss for more stable convergence.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 – Train Loss: 9.9278, Valid RMSE: 1.3532\n",
      "Epoch 2/10 – Train Loss: 1.0362, Valid RMSE: 0.9103\n",
      "Epoch 3/10 – Train Loss: 0.7914, Valid RMSE: 0.8869\n",
      "Epoch 4/10 – Train Loss: 0.7730, Valid RMSE: 0.8815\n",
      "Epoch 5/10 – Train Loss: 0.7670, Valid RMSE: 0.8799\n",
      "Epoch 6/10 – Train Loss: 0.7639, Valid RMSE: 0.8788\n",
      "Epoch 7/10 – Train Loss: 0.7626, Valid RMSE: 0.8787\n",
      "Epoch 8/10 – Train Loss: 0.7610, Valid RMSE: 0.8788\n",
      "Epoch 9/10 – Train Loss: 0.7602, Valid RMSE: 0.8778\n",
      "Epoch 10/10 – Train Loss: 0.7595, Valid RMSE: 0.8782\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Training and Validation Loop with Masking Current Item and RMSE Logging\n",
    "NUM_EPOCHS = 10\n",
    "PAD_IDX = model_svdpp.implicit_emb.padding_idx\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_svdpp.train()\n",
    "    train_loss_total, train_count = 0.0, 0\n",
    "    for sid, pid, ratings in train_loader:\n",
    "        sid = sid.to(device)\n",
    "        pid = pid.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # clone and mask out current item from implicit feedback\n",
    "        imp_items = implicit_matrix[sid].clone()\n",
    "        imp_items[imp_items == pid.unsqueeze(1)] = PAD_IDX\n",
    "\n",
    "        preds = model_svdpp(sid, pid, imp_items)\n",
    "        loss = F.mse_loss(preds, ratings)\n",
    "\n",
    "        optim_svdpp.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_svdpp.step()\n",
    "\n",
    "        train_loss_total += loss.item() * sid.size(0)\n",
    "        train_count += sid.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss_total / train_count\n",
    "\n",
    "    model_svdpp.eval()\n",
    "    val_mse_total, val_count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for sid, pid, ratings in valid_loader:\n",
    "            sid = sid.to(device)\n",
    "            pid = pid.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            imp_items = implicit_matrix[sid].clone()\n",
    "            imp_items[imp_items == pid.unsqueeze(1)] = PAD_IDX\n",
    "\n",
    "            preds = model_svdpp(sid, pid, imp_items)\n",
    "            mse = F.mse_loss(preds, ratings, reduction='sum')\n",
    "\n",
    "            val_mse_total += mse.item()\n",
    "            val_count += sid.size(0)\n",
    "\n",
    "    valid_rmse = (val_mse_total / val_count) ** 0.5\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} – Train Loss: {avg_train_loss:.4f}, Valid RMSE: {valid_rmse:.4f}\")\n",
    "    scheduler.step(val_mse_total / val_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.878\n"
     ]
    }
   ],
   "source": [
    "pred_fn_svdpp = lambda sids, pids: model_svdpp(\n",
    "    torch.from_numpy(sids).to(device),\n",
    "    torch.from_numpy(pids).to(device),\n",
    "    implicit_matrix[\n",
    "        torch.from_numpy(np.clip(sids, 0, implicit_matrix.shape[0] - 1)).to(device)\n",
    "    ]\n",
    ").detach().cpu().numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_ratings_df, pred_fn_svdpp)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we achieved a validation RMSE of 0.878, which again only just beats our simple baseline. \n",
    "\n",
    "However, SVD++ allowed extensive hyperparameter tuning—embedding size, learning rate, weight decay, and scheduler settings—and the current configuration represents the best combination we found. This fine-tuning flexibility, along with the model’s ability to incorporate both biases and implicit feedback, makes SVD++ preferable to the more rigid iterative SVD approach. Additional advantages include faster convergence on sparse data and a straightforward path to extend with dropout or deeper bias structures if needed.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Wishlist Data Incorporation\n",
    "\n",
    "From this point, we incorporate the wishlist information by passing both the ratings DataFrame (with columns `sid`, `pid`, `rating`) and the wishlist DataFrame (with columns `sid`, `pid`) into `get_dataset`. Inside the function, we first tag every wishlist entry with `wishlist = 1.0` and then perform a **full outer join** on `(sid, pid)`, ensuring that any researcher–paper pair present in either list appears exactly once. After merging, we fill missing `rating` or `wishlist` values with `0.0` (indicating no explicit rating or no wishlist flag), convert each column into a PyTorch tensor, and pack them into a `TensorDataset` of `(sid, pid, rating, wishlist_flag)` for downstream training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    wishlist_df: pd.DataFrame = None\n",
    ") -> torch.utils.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns a dataset of (sid, pid, rating, wishlist_flag) for every\n",
    "    (sid,pid) in ratings OR wishlist.  Missing ratings or wishlist\n",
    "    entries are filled with 0.0.\n",
    "    \"\"\"\n",
    "    # If no wishlist provided, just add an empty wishlist column\n",
    "    if wishlist_df is None:\n",
    "        merged = df.copy()\n",
    "        merged['wishlist'] = 0.0\n",
    "\n",
    "    else:\n",
    "        w = wishlist_df[['sid', 'pid']].copy()\n",
    "        w['wishlist'] = 1.0\n",
    "\n",
    "        merged = pd.merge(\n",
    "            df[['sid', 'pid', 'rating']],\n",
    "            w,\n",
    "            on=['sid', 'pid'],\n",
    "            how='outer'       # <-- full join\n",
    "        )\n",
    "\n",
    "        merged['rating'] = merged['rating'].fillna(0.0)\n",
    "        merged['wishlist'] = merged['wishlist'].fillna(0.0)\n",
    "\n",
    "    sids      = torch.from_numpy(merged['sid'].to_numpy()).long()\n",
    "    pids      = torch.from_numpy(merged['pid'].to_numpy()).long()\n",
    "    ratings   = torch.from_numpy(merged['rating'].to_numpy()).float()\n",
    "    wishlists = torch.from_numpy(merged['wishlist'].to_numpy()).float()\n",
    "\n",
    "    return torch.utils.data.TensorDataset(sids, pids, ratings, wishlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Neural Matrix Factorization (NeuMF)\n",
    "\n",
    "We build a hybrid model that learns both linear and nonlinear interactions and jointly predicts ratings and wishlist preferences. NeuMF combines a GMF branch (element-wise product of embeddings) with an MLP branch (deep concatenation of embeddings), fuses them via a learned weight $\\alpha$, and uses separate output heads for rating and wishlist signals.\n",
    "\n",
    "---\n",
    "\n",
    "**Detailed Architecture**\n",
    "\n",
    "- **Input embeddings**  \n",
    "  Each researcher $r$ and paper $p$ has a $d$-dimensional embedding $\\mathbf{u}_r,\\mathbf{v}_p\\in\\mathbb{R}^d$.\n",
    "\n",
    "- **GMF branch**  \n",
    "  $$g = \\mathbf{u}_r \\odot \\mathbf{v}_p$$\n",
    "\n",
    "- **MLP branch**  \n",
    "  Concatenate and feed through hidden layers:  \n",
    "  $$\n",
    "  x^{(1)} = [\\mathbf{u}_r, \\mathbf{v}_p],\\quad\n",
    "  x^{(\\ell+1)} = \\mathrm{GELU}(W^{(\\ell)} x^{(\\ell)} + b^{(\\ell)}),\\;\\ell=1,\\dots,L\n",
    "  $$\n",
    "  yielding $m = x^{(L+1)}$.\n",
    "\n",
    "- **Fusion**  \n",
    "  $$f = [\\,\\alpha\\,g,\\;(1-\\alpha)\\,m\\,],\\quad \\alpha\\in[0,1]\\ \\text{learned}$$\n",
    "\n",
    "- **Output heads**  \n",
    "  - **Rating**:  \n",
    "    $$\\hat r = 1 + 4\\;\\sigma\\bigl(W_r\\,f + b_r\\bigr),$$  \n",
    "    constraining $\\hat r\\in[1,5]$.  \n",
    "  - **Wishlist**:  \n",
    "    $$\\hat w = \\sigma\\bigl(W_w\\,f + b_w\\bigr),\\quad \\hat w\\in[0,1].$$\n",
    "\n",
    "- **Loss function**  \n",
    "  Combine rating MSE (only where no wishlist) and wishlist MSE:  \n",
    "  $$\n",
    "  \\ell = \\frac{1}{2}\\|\\hat r - r\\|^2_{\\!(w=0)} \\;+\\;\\lambda\\,\\|\\hat w - w\\|^2,\n",
    "  $$  \n",
    "  where $\\|\\cdot\\|_{(w=0)}$ masks out entries with $w=1$, and $\\lambda$ is the wishlist weight.\n",
    "\n",
    "This design lets NeuMF capture both simple latent-factor signals and high-order nonlinear interactions, simultaneously optimizing for two complementary tasks.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_researchers, num_papers, emb_dim):\n",
    "        super().__init__()\n",
    "        self.researcher_emb = nn.Embedding(num_researchers, emb_dim)\n",
    "        self.paper_emb = nn.Embedding(num_papers, emb_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, researcher_indices, paper_indices):\n",
    "        r = self.dropout(self.researcher_emb(researcher_indices))\n",
    "        p = self.dropout(self.paper_emb(paper_indices))\n",
    "        return r * p\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_researchers, num_papers, emb_dim, mlp_layers):\n",
    "        super().__init__()\n",
    "        self.researcher_emb = nn.Embedding(num_researchers, emb_dim)\n",
    "        self.paper_emb = nn.Embedding(num_papers, emb_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        layers = []\n",
    "        input_size = emb_dim * 2\n",
    "        for h in mlp_layers:\n",
    "            layers += [nn.Linear(input_size, h), nn.GELU(), nn.Dropout(0.2)]\n",
    "            input_size = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, researcher_indices, paper_indices):\n",
    "        r = self.dropout(self.researcher_emb(researcher_indices))\n",
    "        p = self.dropout(self.paper_emb(paper_indices))\n",
    "        x = torch.cat([r, p], dim=1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_researchers, num_papers, emb_dim, mlp_layers, wishlist_weight=2.0):\n",
    "        super().__init__()\n",
    "        self.gmf = GMF(num_researchers, num_papers, emb_dim)\n",
    "        self.mlp = MLP(num_researchers, num_papers, emb_dim, mlp_layers)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "        self.wishlist_weight = wishlist_weight\n",
    "\n",
    "        final_size = emb_dim + mlp_layers[-1]\n",
    "        self.fc_rating = nn.Linear(final_size, 1)\n",
    "        self.fc_wishlist = nn.Linear(final_size, 1)\n",
    "\n",
    "    def forward(self, researcher_indices, paper_indices):\n",
    "        g = self.gmf(researcher_indices, paper_indices)\n",
    "        m = self.mlp(researcher_indices, paper_indices)\n",
    "        fused = torch.cat([self.alpha * g, (1 - self.alpha) * m], dim=1)\n",
    "        rating = 1 + 4 * torch.sigmoid(self.fc_rating(fused).squeeze(1))\n",
    "        wishlist = torch.sigmoid(self.fc_wishlist(fused).squeeze(1))\n",
    "        return rating, wishlist\n",
    "\n",
    "    def calc_loss(self, rating_pred, rating_target, wishlist_pred, wishlist_target):\n",
    "        # Only compute rating loss where wishlist_target is 0\n",
    "        mask = wishlist_target == 0\n",
    "        rating_loss = F.mse_loss(rating_pred[mask], rating_target[mask])\n",
    "        wishlist_loss = F.mse_loss(wishlist_pred, wishlist_target)\n",
    "        return rating_loss + self.wishlist_weight * wishlist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(train_ratings_df, train_wishlist_df)\n",
    "valid_dataset = get_dataset(valid_ratings_df)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the NeuMF model with $10,000$ researchers and $1,000$ papers, embedding each into a $16$-dimensional latent space and using two MLP hidden layers of size $32$.  \n",
    "\n",
    "The wishlist loss is weighted by $2.0$, and training runs for $40$ epochs with batch size $64$, using AdamW at learning rate $5\\times10^{-3}$ and L2 regularization; a ReduceLROnPlateau scheduler halves the learning rate if validation loss does not improve for one epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USERS = 10_000\n",
    "NUM_ITEMS = 1_000\n",
    "EMB_DIM = 16\n",
    "MLP_HIDDEN = [32, 32]\n",
    "WISHLIST_LOSS_WEIGHT = 2.0\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "model_neumf = NeuMF(NUM_USERS, NUM_ITEMS, EMB_DIM, MLP_HIDDEN, WISHLIST_LOSS_WEIGHT).to(device)\n",
    "optimizer = torch.optim.AdamW(model_neumf.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuMF training loop mirrors the SVD++ procedure: over $40$ epochs we perform forward/backward passes with AdamW on each batch’s ratings and wishlist flags, then compute validation RMSE (clamping ratings to $[1,5]$) and step the ReduceLROnPlateau scheduler.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 – Train Loss: 1.2049, Valid RMSE: 0.9039\n",
      "Epoch 2/40 – Train Loss: 1.1356, Valid RMSE: 0.9056\n",
      "Epoch 3/40 – Train Loss: 1.1269, Valid RMSE: 0.8964\n",
      "Epoch 4/40 – Train Loss: 1.1112, Valid RMSE: 0.8905\n",
      "Epoch 5/40 – Train Loss: 1.0988, Valid RMSE: 0.8889\n",
      "Epoch 6/40 – Train Loss: 1.0920, Valid RMSE: 0.8867\n",
      "Epoch 7/40 – Train Loss: 1.0842, Valid RMSE: 0.8854\n",
      "Epoch 8/40 – Train Loss: 1.0792, Valid RMSE: 0.8851\n",
      "Epoch 9/40 – Train Loss: 1.0761, Valid RMSE: 0.8812\n",
      "Epoch 10/40 – Train Loss: 1.0736, Valid RMSE: 0.8848\n",
      "Epoch 11/40 – Train Loss: 1.0736, Valid RMSE: 0.8797\n",
      "Epoch 12/40 – Train Loss: 1.0719, Valid RMSE: 0.8822\n",
      "Epoch 13/40 – Train Loss: 1.0705, Valid RMSE: 0.8792\n",
      "Epoch 14/40 – Train Loss: 1.0701, Valid RMSE: 0.8748\n",
      "Epoch 15/40 – Train Loss: 1.0704, Valid RMSE: 0.8779\n",
      "Epoch 16/40 – Train Loss: 1.0691, Valid RMSE: 0.8768\n",
      "Epoch 17/40 – Train Loss: 1.0231, Valid RMSE: 0.8610\n",
      "Epoch 18/40 – Train Loss: 1.0144, Valid RMSE: 0.8617\n",
      "Epoch 19/40 – Train Loss: 1.0161, Valid RMSE: 0.8620\n",
      "Epoch 20/40 – Train Loss: 0.9803, Valid RMSE: 0.8534\n",
      "Epoch 21/40 – Train Loss: 0.9721, Valid RMSE: 0.8543\n",
      "Epoch 22/40 – Train Loss: 0.9718, Valid RMSE: 0.8532\n",
      "Epoch 23/40 – Train Loss: 0.9734, Valid RMSE: 0.8553\n",
      "Epoch 24/40 – Train Loss: 0.9743, Valid RMSE: 0.8536\n",
      "Epoch 25/40 – Train Loss: 0.9470, Valid RMSE: 0.8499\n",
      "Epoch 26/40 – Train Loss: 0.9398, Valid RMSE: 0.8499\n",
      "Epoch 27/40 – Train Loss: 0.9389, Valid RMSE: 0.8493\n",
      "Epoch 28/40 – Train Loss: 0.9392, Valid RMSE: 0.8507\n",
      "Epoch 29/40 – Train Loss: 0.9398, Valid RMSE: 0.8504\n",
      "Epoch 30/40 – Train Loss: 0.9213, Valid RMSE: 0.8491\n",
      "Epoch 31/40 – Train Loss: 0.9174, Valid RMSE: 0.8486\n",
      "Epoch 32/40 – Train Loss: 0.9168, Valid RMSE: 0.8490\n",
      "Epoch 33/40 – Train Loss: 0.9162, Valid RMSE: 0.8486\n",
      "Epoch 34/40 – Train Loss: 0.9050, Valid RMSE: 0.8485\n",
      "Epoch 35/40 – Train Loss: 0.9028, Valid RMSE: 0.8485\n",
      "Epoch 36/40 – Train Loss: 0.9015, Valid RMSE: 0.8487\n",
      "Epoch 37/40 – Train Loss: 0.8954, Valid RMSE: 0.8485\n",
      "Epoch 38/40 – Train Loss: 0.8941, Valid RMSE: 0.8486\n",
      "Epoch 39/40 – Train Loss: 0.8919, Valid RMSE: 0.8485\n",
      "Epoch 40/40 – Train Loss: 0.8907, Valid RMSE: 0.8484\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_neumf.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for sid, pid, ratings, wishlists in train_loader:\n",
    "        sid, pid = sid.to(device), pid.to(device)\n",
    "        ratings, wishlists = ratings.to(device), wishlists.to(device)\n",
    "\n",
    "        pred_ratings, pred_wishlist = model_neumf(sid, pid)\n",
    "        loss = model_neumf.calc_loss(pred_ratings, ratings, pred_wishlist, wishlists)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * sid.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model_neumf.eval()\n",
    "    mse_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sid, pid, ratings, _ in valid_loader:\n",
    "            sid, pid = sid.to(device), pid.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            pred_ratings, _ = model_neumf(sid, pid)\n",
    "            mse = F.mse_loss(pred_ratings.clamp(1, 5), ratings)\n",
    "            mse_total += mse.item() * sid.size(0)\n",
    "\n",
    "    rmse = (mse_total / len(valid_loader.dataset)) ** 0.5\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} – Train Loss: {avg_train_loss:.4f}, Valid RMSE: {rmse:.4f}\")\n",
    "    scheduler.step(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/neumf.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model_neumf.state_dict(), \"models/neumf.pt\")\n",
    "print(\"Model saved to models/neumf.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.848\n"
     ]
    }
   ],
   "source": [
    "pred_fn_neumf = lambda sids, pids: model_neumf(\n",
    "    torch.from_numpy(sids).to(device),\n",
    "    torch.from_numpy(pids).to(device)\n",
    ")[0].clamp(1, 5).detach().cpu().numpy()\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_ratings_df, pred_fn_neumf)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuMF model outperformed our previous baselines—marking the first time we beat the hard benchmark—with surprisingly strong validation results.\n",
    "\n",
    "Achieving this required extensive tuning: we set the wishlist loss weight to $2.0$ (since MSE is less severe than binary cross-entropy, allowing a coefficient higher than 1), kept embedding and hidden layer sizes modest to prevent overfitting, and applied a relatively high weight decay $\\lambda$ for regularization. Too large a decay would prevent the model from learning (training loss wouldn’t decrease), so this configuration yielded the best overall performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6: Graph-Augmented NeuMF (GraphNeuMF)\n",
    "\n",
    "We model researcher–paper interactions as a bipartite graph to propagate information across similar nodes. Let \\(A\\) be the adjacency matrix and \\(\\tilde D\\) its degree matrix; LightGCN refines initial embeddings \\(X^{(0)}\\) via\n",
    "\n",
    "$$\n",
    "X^{(l+1)} = \\tilde D^{-1}A\\,X^{(l)},\\qquad l = 0,\\dots,L-1,\n",
    "$$\n",
    "\n",
    "and aggregates them into\n",
    "\n",
    "$$\n",
    "X = \\frac{1}{L+1}\\sum_{l=0}^L X^{(l)}.\n",
    "$$\n",
    "\n",
    "After pretraining with a BPR loss to separate positive and negative edges, we extract the user and paper rows of \\(X\\) and fuse them with GMF’s elementwise product and MLP’s encoding into a combined vector \\(f\\). Two output heads then compute:\n",
    "\n",
    "- **Rating**  \n",
    "  $$\n",
    "  \\hat r = 1 + 4\\,\\sigma\\bigl(W_r f + b_r\\bigr),\n",
    "  $$\n",
    "\n",
    "- **Wishlist**  \n",
    "  $$\n",
    "  \\hat w = \\sigma\\bigl(W_w f + b_w\\bigr).\n",
    "  $$\n",
    "\n",
    "We train both tasks with mean-squared error, weighting the wishlist term by \\(\\lambda_{\\text{wish}}\\):\n",
    "\n",
    "$$\n",
    "\\ell = \\mathrm{MSE}(\\hat r, r)\n",
    "\\;+\\;\n",
    "\\lambda_{\\text{wish}}\\;\\mathrm{MSE}(\\hat w, w).\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.num_users  = num_users\n",
    "        self.num_items  = num_items\n",
    "        self.emb_dim    = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "        # graph convolution layers\n",
    "        from torch_geometric.nn import LGConv\n",
    "        self.convs = nn.ModuleList([\n",
    "            LGConv(normalize=True) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # will hold a SparseTensor\n",
    "        self.adj = None\n",
    "\n",
    "    def set_adj(self, edge_index: torch.LongTensor, device: torch.device):\n",
    "        # build and store the adjacency once\n",
    "        self.adj = SparseTensor(\n",
    "            row=edge_index[0],\n",
    "            col=edge_index[1] + self.num_users,\n",
    "            sparse_sizes=(self.num_users + self.num_items,) * 2\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self):\n",
    "        x = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embeddings = [x]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, self.adj)\n",
    "            embeddings.append(x)\n",
    "\n",
    "        x = torch.mean(torch.stack(embeddings), dim=0)\n",
    "        user_emb, item_emb = torch.split(x, [self.num_users, self.num_items])\n",
    "        return user_emb, item_emb\n",
    "\n",
    "    def get_embeddings(self, user_idx, item_idx):\n",
    "        return self.user_emb(user_idx), self.item_emb(item_idx)\n",
    "    \n",
    "    \n",
    "class GraphNeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim, mlp_layers, wishlist_weight):\n",
    "        super().__init__()\n",
    "        self.lightgcn       = LightGCN(num_users, num_items, emb_dim)\n",
    "        self.gmf            = GMF(num_users, num_items, emb_dim)\n",
    "        self.mlp            = MLP(num_users, num_items, emb_dim, mlp_layers)\n",
    "        self.alpha          = nn.Parameter(torch.tensor(0.5))\n",
    "        self.wishlist_weight = wishlist_weight\n",
    "\n",
    "        final_size = emb_dim + mlp_layers[-1] + emb_dim + emb_dim\n",
    "        self.fc_rating  = nn.Linear(final_size, 1)\n",
    "        self.fc_wishlist = nn.Linear(final_size, 1)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_emb_g, item_emb_g = self.lightgcn.get_embeddings(user_idx, item_idx)\n",
    "        gmf_out  = self.gmf(user_idx, item_idx)\n",
    "        mlp_out  = self.mlp(user_idx, item_idx)\n",
    "\n",
    "        fused = torch.cat([\n",
    "            self.alpha * gmf_out,\n",
    "            (1.0 - self.alpha) * mlp_out,\n",
    "            user_emb_g,\n",
    "            item_emb_g\n",
    "        ], dim=1)\n",
    "        fused = fused if fused.dim() > 1 else fused.unsqueeze(0)\n",
    "\n",
    "        rating_pred   = 1.0 + 4.0 * torch.sigmoid(self.fc_rating(fused).squeeze(1))\n",
    "        wishlist_prob = torch.sigmoid(self.fc_wishlist(fused).squeeze(1))\n",
    "        return rating_pred, wishlist_prob\n",
    "\n",
    "    def calc_loss(self, rating_pred, rating_target, wishlist_pred, wishlist_target):\n",
    "        mask_r = (wishlist_target == 0.0)\n",
    "        rating_loss   = F.mse_loss(rating_pred[mask_r], rating_target[mask_r])\n",
    "        wishlist_loss = F.mse_loss(wishlist_pred, wishlist_target)\n",
    "        return rating_loss + self.wishlist_weight * wishlist_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This constructs a bipartite edge list by stacking researcher and paper indices—optionally filtering to only wishlist interactions—into a 2×E tensor (`edge_index`) for graph-based models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interaction_graph_from_dataset(\n",
    "    dataset: torch.utils.data.TensorDataset,\n",
    "    use_wishlist: bool = True\n",
    ") -> torch.LongTensor:\n",
    "    sids, pids, ratings, wishlists = dataset.tensors\n",
    "    if use_wishlist:\n",
    "        mask = wishlists == 1.0\n",
    "    else:\n",
    "        mask = torch.ones_like(wishlists, dtype=torch.bool)\n",
    "\n",
    "    edge_index = torch.stack([sids[mask], pids[mask]], dim=0)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(train_ratings_df, train_wishlist_df)\n",
    "valid_dataset = get_dataset(valid_ratings_df)\n",
    "\n",
    "edge_index = build_interaction_graph_from_dataset(train_dataset, use_wishlist=False).to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure GraphNeuMF with $10,000$ researchers and $1,000$ papers, embedding each into a $16$-dimensional space and using two MLP hidden layers of size 32. \n",
    "\n",
    "The wishlist loss weight is set to 2.0 (as with NeuMf where we have fined-tunned this hyperparameter), batch size to 64, and learning rate to $10^{-3}$. \n",
    "\n",
    "Before joint training, we precompute and set the LightGCN adjacency (`edge_index`) to enable efficient graph convolutions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USERS       = 10_000\n",
    "NUM_ITEMS       = 1_000\n",
    "EMB_DIM         = 16\n",
    "MLP_LAYERS      = [32, 32]\n",
    "WISHLIST_WEIGHT = 2.0\n",
    "BATCH_SIZE      = 64\n",
    "LEARNING_RATE   = 1e-3\n",
    "\n",
    "model_graphneumf = GraphNeuMF(\n",
    "    num_users=NUM_USERS,\n",
    "    num_items=NUM_ITEMS,\n",
    "    emb_dim=EMB_DIM,\n",
    "    mlp_layers=MLP_LAYERS,\n",
    "    wishlist_weight=2.0\n",
    ").to(device)\n",
    "\n",
    "# precompute adjacency\n",
    "model_graphneumf.lightgcn.set_adj(edge_index, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pretrain the LightGCN layers for $5$ epochs using a BPR-style loss that encourages higher scores for observed researcher–paper edges than for randomly sampled negatives. This step refines the graph-based embeddings before they are fused into the GraphNeuMF model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training LightGCN...\n",
      " Pre-train Epoch 1/5  Loss=0.6931\n",
      " Pre-train Epoch 2/5  Loss=0.6931\n",
      " Pre-train Epoch 3/5  Loss=0.6931\n",
      " Pre-train Epoch 4/5  Loss=0.6931\n",
      " Pre-train Epoch 5/5  Loss=0.6931\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-training LightGCN...\")\n",
    "pretrain_opt       = torch.optim.Adam(model_graphneumf.lightgcn.parameters(), lr=0.01)\n",
    "GCN_PRETRAIN_EPOCHS= 5\n",
    "\n",
    "for epoch in range(GCN_PRETRAIN_EPOCHS):\n",
    "    model_graphneumf.lightgcn.train()\n",
    "    pretrain_opt.zero_grad()\n",
    "\n",
    "    user_emb, item_emb = model_graphneumf.lightgcn()\n",
    "    pos_u, pos_i       = edge_index\n",
    "    neg_i              = torch.randint(0, NUM_ITEMS, (edge_index.size(1)*10,), device=device)\n",
    "\n",
    "    pos_scores = (user_emb[pos_u] * item_emb[pos_i]).sum(dim=1)\n",
    "    neg_scores = (user_emb[pos_u.repeat(10)] * item_emb[neg_i]).sum(dim=1)\n",
    "\n",
    "    loss = -torch.log(torch.sigmoid(pos_scores.repeat(10) - neg_scores)).mean()\n",
    "    loss.backward()\n",
    "    pretrain_opt.step()\n",
    "\n",
    "    print(f\" Pre-train Epoch {epoch+1}/{GCN_PRETRAIN_EPOCHS}  Loss={loss.item():.4f}\")\n",
    "    if loss.item() < 0.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GraphNeuMF training loop is almost identical to NeuMF’s: over $40$ epochs we batch researcher–paper pairs (and wishlist flags), compute the combined rating and wishlist loss, backpropagate with AdamW, and step a ReduceLROnPlateau scheduler based on validation RMSE.  \n",
    "\n",
    "This end-to-end training fine-tunes both the graph-based embeddings and the hybrid NeuMF components together, yielding the final GraphNeuMF model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_graphneumf.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/40] Train loss=1.1383, Valid RMSE=0.9000\n",
      "[Epoch 2/40] Train loss=1.0966, Valid RMSE=0.8970\n",
      "[Epoch 3/40] Train loss=1.0899, Valid RMSE=0.8969\n",
      "[Epoch 4/40] Train loss=1.0821, Valid RMSE=0.8969\n",
      "[Epoch 5/40] Train loss=1.0704, Valid RMSE=0.8965\n",
      "[Epoch 6/40] Train loss=1.0524, Valid RMSE=0.8931\n",
      "[Epoch 7/40] Train loss=1.0249, Valid RMSE=0.8857\n",
      "[Epoch 8/40] Train loss=1.0001, Valid RMSE=0.8788\n",
      "[Epoch 9/40] Train loss=0.9861, Valid RMSE=0.8733\n",
      "[Epoch 10/40] Train loss=0.9767, Valid RMSE=0.8699\n",
      "[Epoch 11/40] Train loss=0.9688, Valid RMSE=0.8669\n",
      "[Epoch 12/40] Train loss=0.9642, Valid RMSE=0.8644\n",
      "[Epoch 13/40] Train loss=0.9592, Valid RMSE=0.8619\n",
      "[Epoch 14/40] Train loss=0.9571, Valid RMSE=0.8609\n",
      "[Epoch 15/40] Train loss=0.9532, Valid RMSE=0.8597\n",
      "[Epoch 16/40] Train loss=0.9505, Valid RMSE=0.8580\n",
      "[Epoch 17/40] Train loss=0.9483, Valid RMSE=0.8580\n",
      "[Epoch 18/40] Train loss=0.9463, Valid RMSE=0.8569\n",
      "[Epoch 19/40] Train loss=0.9446, Valid RMSE=0.8567\n",
      "[Epoch 20/40] Train loss=0.9430, Valid RMSE=0.8572\n",
      "[Epoch 21/40] Train loss=0.9419, Valid RMSE=0.8558\n",
      "[Epoch 22/40] Train loss=0.9413, Valid RMSE=0.8555\n",
      "[Epoch 23/40] Train loss=0.9393, Valid RMSE=0.8561\n",
      "[Epoch 24/40] Train loss=0.9380, Valid RMSE=0.8558\n",
      "[Epoch 25/40] Train loss=0.9078, Valid RMSE=0.8527\n",
      "[Epoch 26/40] Train loss=0.9010, Valid RMSE=0.8528\n",
      "[Epoch 27/40] Train loss=0.8993, Valid RMSE=0.8532\n",
      "[Epoch 28/40] Train loss=0.8805, Valid RMSE=0.8529\n",
      "[Epoch 29/40] Train loss=0.8763, Valid RMSE=0.8529\n",
      "[Epoch 30/40] Train loss=0.8657, Valid RMSE=0.8528\n",
      "[Epoch 31/40] Train loss=0.8635, Valid RMSE=0.8528\n",
      "[Epoch 32/40] Train loss=0.8569, Valid RMSE=0.8530\n",
      "[Epoch 33/40] Train loss=0.8569, Valid RMSE=0.8529\n",
      "[Epoch 34/40] Train loss=0.8530, Valid RMSE=0.8530\n",
      "[Epoch 35/40] Train loss=0.8520, Valid RMSE=0.8532\n",
      "[Epoch 36/40] Train loss=0.8505, Valid RMSE=0.8531\n",
      "[Epoch 37/40] Train loss=0.8507, Valid RMSE=0.8531\n",
      "[Epoch 38/40] Train loss=0.8501, Valid RMSE=0.8530\n",
      "[Epoch 39/40] Train loss=0.8494, Valid RMSE=0.8530\n",
      "[Epoch 40/40] Train loss=0.8487, Valid RMSE=0.8529\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model_graphneumf.train()\n",
    "    tot_loss, tot_n = 0.0, 0\n",
    "\n",
    "    for sid, pid, ratings, wishlists in train_loader:\n",
    "        sid, pid = sid.to(device), pid.to(device)\n",
    "        ratings, wishlists = ratings.to(device), wishlists.to(device)\n",
    "\n",
    "        pr, pw = model_graphneumf(sid, pid)\n",
    "        loss   = model_graphneumf.calc_loss(pr, ratings, pw, wishlists)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += loss.item() * sid.size(0)\n",
    "        tot_n    += sid.size(0)\n",
    "\n",
    "    avg_train_loss = tot_loss / tot_n\n",
    "\n",
    "    # validation\n",
    "    model_graphneumf.eval()\n",
    "    val_mse, val_n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for sid, pid, ratings, _ in valid_loader:\n",
    "            sid, pid   = sid.to(device), pid.to(device)\n",
    "            ratings    = ratings.to(device)\n",
    "            pred_r, _  = model_graphneumf(sid, pid)\n",
    "            pred_r_cl  = pred_r.clamp(1.0, 5.0)\n",
    "            val_mse   += F.mse_loss(pred_r_cl, ratings, reduction='sum').item()\n",
    "            val_n     += sid.size(0)\n",
    "\n",
    "    valid_rmse = (val_mse / val_n) ** 0.5\n",
    "    print(f\"[Epoch {epoch}/{NUM_EPOCHS}] \"\n",
    "          f\"Train loss={avg_train_loss:.4f}, Valid RMSE={valid_rmse:.4f}\")\n",
    "\n",
    "    scheduler.step(valid_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.853\n"
     ]
    }
   ],
   "source": [
    "pred_fn_graphneumf = lambda sids, pids: model_graphneumf(\n",
    "    torch.from_numpy(sids).to(device),\n",
    "    torch.from_numpy(pids).to(device)\n",
    ")[0].clamp(1, 5).detach().cpu().numpy()\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_ratings_df, pred_fn_graphneumf)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although GraphNeuMF did not improve on the validation RMSE of $0.848$ achieved by NeuMF alone, its longer training schedule limited our ability to fine-tune its hyperparameters. \n",
    "\n",
    "We anticipate that GraphNeuMF explores complementary regions of the embedding space compared to NeuMF, making their combination in a future ensemble potentially more powerful.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6: Deep Matrix Factorization (DMF)\n",
    "\n",
    "Deep Matrix Factorization (DMF) replaces shallow embeddings with two “towers” of nonlinear transformations, one for researchers and one for papers.  Each tower maps initial embeddings into a deep latent space, and predictions are made by combining these deep representations.\n",
    "\n",
    "Specifically, let  \n",
    "$$\n",
    "\\mathbf{e}_r \\in \\mathbb{R}^d,\\quad \\mathbf{e}_p \\in \\mathbb{R}^d\n",
    "$$  \n",
    "be the researcher and paper embeddings.  We apply $L$ layers of transformations:  \n",
    "$$\n",
    "\\mathbf{u} = f_r(\\mathbf{e}_r),\\quad\n",
    "\\mathbf{v} = f_p(\\mathbf{e}_p),\n",
    "$$  \n",
    "where each $f(\\cdot)$ is a sequence of linear + GELU blocks.  The predicted rating is  \n",
    "$$\n",
    "\\hat r_{rp} = \\mathbf{u}^\\top \\mathbf{v} \\;+\\; b_r \\;+\\; b_p \\;+\\; \\beta,\n",
    "$$  \n",
    "with researcher bias $b_r$, paper bias $b_p$, and global bias $\\beta$.  Implicit feedback is modeled via a BPR loss on wishlist logits, and the overall objective combines MSE on observed ratings with weighted BPR plus embedding regularization:  \n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{MSE}(\\hat r, r) \\;+\\;\\lambda_\\text{wish}\\,\\mathrm{BPR}(\\ell^+, \\ell^-) \\;+\\;\\lambda_\\text{emb}\\|\\mathbf{e}\\|_2^2.\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_block(in_f, out_f):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_f, out_f),\n",
    "        nn.GELU(),\n",
    "    )\n",
    "\n",
    "class DMF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_items,\n",
    "        layers,\n",
    "        wishlist_weight=0.5,\n",
    "        max_rating=5.0,\n",
    "        embed_lambda=1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_rating           = max_rating\n",
    "        self.wishlist_weight = wishlist_weight\n",
    "        self.embed_lambda         = embed_lambda\n",
    "\n",
    "        emb_dim = layers[0]\n",
    "        self.u_emb       = nn.Embedding(num_users, emb_dim)\n",
    "        self.i_emb       = nn.Embedding(num_items, emb_dim)\n",
    "        self.u_bias      = nn.Embedding(num_users, 1)\n",
    "        self.i_bias      = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.user_tower = nn.Sequential(*[\n",
    "            make_block(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)\n",
    "        ])\n",
    "        self.item_tower = nn.Sequential(*[\n",
    "            make_block(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)\n",
    "        ])\n",
    "\n",
    "        self.fc_wish = nn.Linear(layers[-1], 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def bpr_loss(pos_score, neg_score):\n",
    "        return -F.logsigmoid(pos_score - neg_score).mean()\n",
    "\n",
    "    def forward(self, u_idx, i_idx):\n",
    "        u = self.dropout(self.u_emb(u_idx))\n",
    "        v = self.dropout(self.i_emb(i_idx))\n",
    "\n",
    "        u = self.user_tower(u)\n",
    "        v = self.item_tower(v)\n",
    "\n",
    "        x = u * v\n",
    "\n",
    "        dot    = x.sum(dim=1)\n",
    "        ub     = self.u_bias(u_idx).squeeze(1)\n",
    "        ib     = self.i_bias(i_idx).squeeze(1)\n",
    "        rating = dot + ub + ib + self.global_bias\n",
    "\n",
    "        wish_logit = self.fc_wish(x).squeeze(1)\n",
    "        wish       = torch.sigmoid(wish_logit)\n",
    "\n",
    "        return rating, wish, wish_logit\n",
    "\n",
    "    def calc_loss(\n",
    "        self,\n",
    "        rating_pred,\n",
    "        rating_true,\n",
    "        wish_logit_pos=None,\n",
    "        wish_logit_neg=None,\n",
    "    ):\n",
    "        mask = rating_true > 0\n",
    "        rating_loss = F.mse_loss(rating_pred[mask], rating_true[mask])\n",
    "\n",
    "        if wish_logit_pos is None or wish_logit_neg is None:\n",
    "            imp_loss = torch.tensor(0., device=rating_pred.device, dtype=rating_pred.dtype)\n",
    "        else:\n",
    "            imp_loss = self.bpr_loss(wish_logit_pos, wish_logit_neg)\n",
    "\n",
    "        base = rating_loss + self.wishlist_weight * imp_loss\n",
    "\n",
    "        emb_reg = (\n",
    "            self.u_emb.weight.norm(p=2).pow(2)\n",
    "          + self.i_emb.weight.norm(p=2).pow(2)\n",
    "        ) * self.embed_lambda\n",
    "\n",
    "        return base + 0.5 * emb_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(train_ratings_df, train_wishlist_df)\n",
    "valid_dataset = get_dataset(valid_ratings_df)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize DMF for $10{,}000$ researchers and $1{,}000$ papers with tower layer sizes $[32,64,128]$, embedding dimension $32$ (which is the first tower dim), and a wishlist weight of $0.5$. \n",
    "\n",
    "The AdamW optimizer uses a learning rate of $10^{-3}$, weight decay of $10^{-3}$, and embedding regularization $\\lambda_{\\text{emb}}=10^{-4}$. \n",
    "\n",
    "A ReduceLROnPlateau scheduler halves the learning rate if validation RMSE does not improve for one epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USERS   = 10000\n",
    "NUM_ITEMS   = 1000\n",
    "MLP_HIDDEN  = [32, 64, 128]\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY  = 1e-3\n",
    "EMBED_LAMBDA  = 1e-4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dmf = DMF(NUM_USERS, NUM_ITEMS, MLP_HIDDEN, wishlist_weight=0.5, max_rating=5.0, embed_lambda=EMBED_LAMBDA).to(device)\n",
    "optimizer = torch.optim.AdamW(model_dmf.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DMF training loop follows the same pattern as NeuMF and GraphNeuMF—batch-wise forward/backward passes with AdamW and ReduceLROnPlateau—while additionally performing BPR-style negative sampling: for each positive wishlist example we sample a random negative paper and compute a pairwise loss $$\\ell_{\\text{BPR}} = -\\log\\sigma(s^+ - s^-)$$ to encourage higher scores for true interactions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 – Train Loss: 3.5817, Valid RMSE: 0.9294\n",
      "Epoch 2/40 – Train Loss: 1.2795, Valid RMSE: 0.9193\n",
      "Epoch 3/40 – Train Loss: 1.1961, Valid RMSE: 0.9053\n",
      "Epoch 4/40 – Train Loss: 1.1603, Valid RMSE: 0.8953\n",
      "Epoch 5/40 – Train Loss: 1.1356, Valid RMSE: 0.8904\n",
      "Epoch 6/40 – Train Loss: 1.1162, Valid RMSE: 0.8825\n",
      "Epoch 7/40 – Train Loss: 1.0984, Valid RMSE: 0.8771\n",
      "Epoch 8/40 – Train Loss: 1.0862, Valid RMSE: 0.8760\n",
      "Epoch 9/40 – Train Loss: 1.0817, Valid RMSE: 0.8732\n",
      "Epoch 10/40 – Train Loss: 1.0798, Valid RMSE: 0.8746\n",
      "Epoch 11/40 – Train Loss: 1.0786, Valid RMSE: 0.8732\n",
      "Epoch 12/40 – Train Loss: 1.0408, Valid RMSE: 0.8611\n",
      "Epoch 13/40 – Train Loss: 1.0272, Valid RMSE: 0.8594\n",
      "Epoch 14/40 – Train Loss: 1.0246, Valid RMSE: 0.8575\n",
      "Epoch 15/40 – Train Loss: 1.0219, Valid RMSE: 0.8560\n",
      "Epoch 16/40 – Train Loss: 1.0200, Valid RMSE: 0.8555\n",
      "Epoch 17/40 – Train Loss: 1.0186, Valid RMSE: 0.8551\n",
      "Epoch 18/40 – Train Loss: 1.0176, Valid RMSE: 0.8534\n",
      "Epoch 19/40 – Train Loss: 1.0150, Valid RMSE: 0.8536\n",
      "Epoch 20/40 – Train Loss: 1.0143, Valid RMSE: 0.8531\n",
      "Epoch 21/40 – Train Loss: 1.0122, Valid RMSE: 0.8535\n",
      "Epoch 22/40 – Train Loss: 1.0121, Valid RMSE: 0.8522\n",
      "Epoch 23/40 – Train Loss: 1.0106, Valid RMSE: 0.8517\n",
      "Epoch 24/40 – Train Loss: 1.0090, Valid RMSE: 0.8510\n",
      "Epoch 25/40 – Train Loss: 1.0085, Valid RMSE: 0.8502\n",
      "Epoch 26/40 – Train Loss: 1.0078, Valid RMSE: 0.8498\n",
      "Epoch 27/40 – Train Loss: 1.0072, Valid RMSE: 0.8490\n",
      "Epoch 28/40 – Train Loss: 1.0065, Valid RMSE: 0.8488\n",
      "Epoch 29/40 – Train Loss: 1.0057, Valid RMSE: 0.8493\n",
      "Epoch 30/40 – Train Loss: 1.0040, Valid RMSE: 0.8491\n",
      "Epoch 31/40 – Train Loss: 0.9642, Valid RMSE: 0.8439\n",
      "Epoch 32/40 – Train Loss: 0.9492, Valid RMSE: 0.8439\n",
      "Epoch 33/40 – Train Loss: 0.9446, Valid RMSE: 0.8436\n",
      "Epoch 34/40 – Train Loss: 0.9418, Valid RMSE: 0.8441\n",
      "Epoch 35/40 – Train Loss: 0.9405, Valid RMSE: 0.8438\n",
      "Epoch 36/40 – Train Loss: 0.9117, Valid RMSE: 0.8438\n",
      "Epoch 37/40 – Train Loss: 0.9023, Valid RMSE: 0.8438\n",
      "Epoch 38/40 – Train Loss: 0.8828, Valid RMSE: 0.8455\n",
      "Epoch 39/40 – Train Loss: 0.8788, Valid RMSE: 0.8454\n",
      "Epoch 40/40 – Train Loss: 0.8652, Valid RMSE: 0.8466\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_dmf.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for sid, pid, ratings, wishlists in train_loader:\n",
    "        sid, pid   = sid.to(device), pid.to(device)\n",
    "        ratings    = ratings.to(device)\n",
    "        wishlists  = wishlists.to(device)\n",
    "\n",
    "        pred_ratings, _, wish_logit = model_dmf(sid, pid)\n",
    "\n",
    "        # BPR negative sampling using wishlist flag\n",
    "        pos_idx = (wishlists > 0).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() > 0:\n",
    "            pos_scores = wish_logit[pos_idx]\n",
    "            u_pos      = sid[pos_idx]\n",
    "            neg_items  = torch.randint(0, NUM_ITEMS, (pos_idx.size(0),), device=device)\n",
    "            _, _, neg_logits = model_dmf(u_pos, neg_items)\n",
    "        else:\n",
    "            pos_scores, neg_logits = None, None\n",
    "\n",
    "        loss = model_dmf.calc_loss(\n",
    "            pred_ratings, ratings,\n",
    "            pos_scores, neg_logits\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * sid.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    model_dmf.eval()\n",
    "    mse_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sid, pid, ratings, _ in valid_loader:\n",
    "            sid, pid   = sid.to(device), pid.to(device)\n",
    "            ratings    = ratings.to(device)\n",
    "            pred_ratings, _, _ = model_dmf(sid, pid)\n",
    "            pred_ratings       = pred_ratings.clamp(1, model_dmf.max_rating)\n",
    "            mse_sum += F.mse_loss(pred_ratings, ratings, reduction='sum').item()\n",
    "\n",
    "    rmse = (mse_sum / len(valid_loader.dataset)) ** 0.5\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} – Train Loss: {avg_train_loss:.4f}, Valid RMSE: {rmse:.4f}\")\n",
    "    scheduler.step(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.847\n"
     ]
    }
   ],
   "source": [
    "pred_fn_dmf = lambda sids, pids: (\n",
    "    model_dmf(\n",
    "        torch.from_numpy(sids).long().to(device),\n",
    "        torch.from_numpy(pids).long().to(device)\n",
    "    )[0].detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_ratings_df, pred_fn_dmf)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DMF model reaches validation RMSE comparable to NeuMF but trains much faster, making it well-suited for quick experimentation. We switched to a combined MSE + BPR objective—$\\ell_{\\text{BPR}} = -\\log\\sigma(s^+ - s^-)$—to better leverage implicit wishlist signals, and added a global bias term $\\beta$ to center our predictions.\n",
    "\n",
    "To prevent overfitting, we increased the embedding regularization to $\\lambda_{\\text{emb}}=10^{-4}$ and applied a weight decay of $10^{-3}$ across all parameters. The deep tower architecture ([32, 64, 128]) provides greater representational power, while these regularization settings ensure the model generalizes effectively. \n",
    "\n",
    "All the hyperparameters were greatly fined-tuned for the best performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together: Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather our four base predictors—NeuMF, DMF, SVD++, and GraphNeuMF—into a dictionary for easy access, and extract the true ratings from the training and validation sets to serve as targets for ensemble weighting.  \n",
    "\n",
    "This setup allows us to experiment with various blending strategies (simple average, weighted average, stacking) by evaluating each predictor’s outputs against `true_ratings_valid`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_functions = {\n",
    "    \"NeuMF\": pred_fn_neumf,\n",
    "    \"DMF\": pred_fn_dmf,\n",
    "    \"SVD++\": pred_fn_svdpp, \n",
    "    \"GraphNeuMF\": pred_fn_graphneumf,\n",
    "}\n",
    "\n",
    "true_ratings_train = train_ratings_df[\"rating\"].values\n",
    "true_ratings_valid = valid_ratings_df[\"rating\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(df, model_name):\n",
    "    sids, pids = df[\"sid\"].values, df[\"pid\"].values\n",
    "    return prediction_functions[model_name](sids, pids)\n",
    "\n",
    "train_predictions = {name: get_model_predictions(train_ratings_df, name) for name in prediction_functions}\n",
    "valid_predictions = {name: get_model_predictions(valid_ratings_df, name) for name in prediction_functions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get each model raw performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuMF Validation RMSE: 0.8484\n",
      "DMF Validation RMSE: 0.8475\n",
      "SVD++ Validation RMSE: 0.8782\n",
      "GraphNeuMF Validation RMSE: 0.8519\n"
     ]
    }
   ],
   "source": [
    "base_model_rmses = {\n",
    "    name: np.sqrt(np.mean((valid_predictions[name] - true_ratings_valid) ** 2))\n",
    "    for name in prediction_functions\n",
    "}\n",
    "\n",
    "for name, rmse in base_model_rmses.items():\n",
    "    print(f\"{name} Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "best_base_model = min(base_model_rmses, key=base_model_rmses.get)\n",
    "best_base_rmse = base_model_rmses[best_base_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple averaging, we compute  \n",
    "$$\n",
    "\\hat r_i \\;=\\; \\frac{1}{M}\\sum_{m=1}^M \\hat r_i^{(m)}\n",
    "$$  \n",
    "and report its RMSE.\n",
    "\n",
    "For weighted averaging, we define  \n",
    "$$\n",
    "w_m \\;=\\; \\frac{1/\\mathrm{RMSE}_m}{\\sum_{j=1}^M (1/\\mathrm{RMSE}_j)},\n",
    "\\quad\n",
    "\\hat r_i \\;=\\; \\sum_{m=1}^M w_m\\,\\hat r_i^{(m)}\n",
    "$$  \n",
    "so that more accurate models carry greater influence, and then compute the resulting RMSE.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average RMSE: 0.8371\n",
      "Weighted Average RMSE: 0.8370\n"
     ]
    }
   ],
   "source": [
    "# Simple Average\n",
    "avg_preds = np.mean(list(valid_predictions.values()), axis=0)\n",
    "avg_rmse = np.sqrt(np.mean((avg_preds - true_ratings_valid) ** 2))\n",
    "print(f\"Simple Average RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "# Weighted Average\n",
    "inv_rmses = {k: 1/v for k, v in base_model_rmses.items()}\n",
    "norm = sum(inv_rmses.values())\n",
    "weights = {k: v / norm for k, v in inv_rmses.items()}\n",
    "\n",
    "weighted_preds = sum(weights[k] * valid_predictions[k] for k in weights)\n",
    "weighted_rmse = np.sqrt(np.mean((weighted_preds - true_ratings_valid) ** 2))\n",
    "print(f\"Weighted Average RMSE: {weighted_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct meta-features for each example by stacking the two base predictions and their average and difference into  \n",
    "$$\n",
    "\\mathbf{x}_i = \\bigl[\\hat r_i^{(1)},\\;\\hat r_i^{(2)},\\;\\tfrac{\\hat r_i^{(1)}+\\hat r_i^{(2)}}{2},\\;\\hat r_i^{(1)}-\\hat r_i^{(2)}\\bigr]^\\top,\n",
    "$$  \n",
    "then standardize each feature to zero mean and unit variance.\n",
    "\n",
    "\n",
    "A small DeepStacking neural network maps $$\\mathbf{x}'_i\\mapsto\\hat r_i$$ through one hidden layer of size $4\\times dim$ and a final output neuron, training to minimize  \n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N\\bigl(\\hat r_i - r_i\\bigr)^2.\n",
    "$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_meta_features(pred_dict):\n",
    "    preds = list(pred_dict.values())\n",
    "    return np.vstack([\n",
    "        preds[0],\n",
    "        preds[1],\n",
    "        np.mean(preds, axis=0),\n",
    "        preds[0] - preds[1]\n",
    "    ]).T\n",
    "\n",
    "X_train = create_meta_features(train_predictions)\n",
    "X_valid = create_meta_features(valid_predictions)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(true_ratings_train, dtype=torch.float32).to(device)\n",
    "y_valid_tensor = torch.tensor(true_ratings_valid, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepStacking(nn.Module):\n",
    "    def __init__(self, in_dim, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        h1 = 4 * in_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, h1),\n",
    "            nn.BatchNorm1d(h1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(h1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# model, optimizer, loss\n",
    "stack_model = DeepStacking(X_train.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(stack_model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# scheduler: reduce LR on plateau of val RMSE\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',           # we want to minimize RMSE\n",
    "    factor=0.5,           # LR = LR * factor\n",
    "    patience=1 ,          # wait 10 epochs before reducing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Train RMSE: 3.7671 | Val   RMSE: 2.9919\n",
      "Epoch  250 | Train RMSE: 1.9367 | Val   RMSE: 1.9380\n",
      "Epoch  500 | Train RMSE: 1.3347 | Val   RMSE: 1.3447\n",
      "Epoch  750 | Train RMSE: 1.1224 | Val   RMSE: 1.1728\n",
      "Epoch 1000 | Train RMSE: 0.9369 | Val   RMSE: 1.0311\n",
      "Epoch 1250 | Train RMSE: 0.9020 | Val   RMSE: 0.9991\n",
      "Epoch 1500 | Train RMSE: 0.7822 | Val   RMSE: 0.9174\n",
      "Epoch 1750 | Train RMSE: 0.7306 | Val   RMSE: 0.8764\n",
      "Epoch 2000 | Train RMSE: 0.7201 | Val   RMSE: 0.8689\n",
      "Epoch 2250 | Train RMSE: 0.7189 | Val   RMSE: 0.8669\n",
      "Epoch 2500 | Train RMSE: 0.7166 | Val   RMSE: 0.8637\n",
      "Epoch 2750 | Train RMSE: 0.7150 | Val   RMSE: 0.8631\n",
      "Epoch 3000 | Train RMSE: 0.7144 | Val   RMSE: 0.8622\n",
      "Epoch 3250 | Train RMSE: 0.7138 | Val   RMSE: 0.8631\n",
      "Epoch 3500 | Train RMSE: 0.7131 | Val   RMSE: 0.8606\n",
      "Epoch 3750 | Train RMSE: 0.7126 | Val   RMSE: 0.8601\n",
      "Epoch 4000 | Train RMSE: 0.7117 | Val   RMSE: 0.8593\n",
      "Epoch 4250 | Train RMSE: 0.7119 | Val   RMSE: 0.8588\n",
      "Epoch 4500 | Train RMSE: 0.7202 | Val   RMSE: 0.8655\n",
      "Epoch 4750 | Train RMSE: 0.7115 | Val   RMSE: 0.8586\n",
      "Epoch 5000 | Train RMSE: 0.7103 | Val   RMSE: 0.8579\n",
      "Epoch 5250 | Train RMSE: 0.7111 | Val   RMSE: 0.8592\n",
      "Epoch 5500 | Train RMSE: 0.7110 | Val   RMSE: 0.8588\n",
      "Epoch 5750 | Train RMSE: 0.7109 | Val   RMSE: 0.8590\n",
      "Epoch 6000 | Train RMSE: 0.7104 | Val   RMSE: 0.8588\n",
      "Epoch 6250 | Train RMSE: 0.7110 | Val   RMSE: 0.8588\n",
      "Epoch 6500 | Train RMSE: 0.7110 | Val   RMSE: 0.8591\n",
      "Epoch 6750 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 7000 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 7250 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 7500 | Train RMSE: 0.7109 | Val   RMSE: 0.8592\n",
      "Epoch 7750 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 8000 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 8250 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 8500 | Train RMSE: 0.7109 | Val   RMSE: 0.8592\n",
      "Epoch 8750 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 9000 | Train RMSE: 0.7109 | Val   RMSE: 0.8592\n",
      "Epoch 9250 | Train RMSE: 0.7109 | Val   RMSE: 0.8591\n",
      "Epoch 9500 | Train RMSE: 0.7109 | Val   RMSE: 0.8592\n",
      "Epoch 9750 | Train RMSE: 0.7109 | Val   RMSE: 0.8592\n"
     ]
    }
   ],
   "source": [
    "best_rmse = float(\"inf\")\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # ——— Training step ———\n",
    "    stack_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = stack_model(X_train_tensor)\n",
    "    loss = loss_fn(pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ——— Every 250 epochs: evaluate both train and val ———\n",
    "    if epoch % 250 == 0:\n",
    "        stack_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # training RMSE\n",
    "            train_preds = stack_model(X_train_tensor)\n",
    "            train_rmse  = torch.sqrt(loss_fn(train_preds, y_train_tensor)).item()\n",
    "\n",
    "            # validation RMSE (with clamp if desired)\n",
    "            val_preds = stack_model(X_valid_tensor).clamp(1, 5)\n",
    "            val_rmse  = torch.sqrt(loss_fn(val_preds, y_valid_tensor)).item()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch:4d} | \"\n",
    "                f\"Train RMSE: {train_rmse:.4f} | \"\n",
    "                f\"Val   RMSE: {val_rmse:.4f}\"\n",
    "            )\n",
    "\n",
    "            scheduler.step(val_rmse)\n",
    "\n",
    "            # checkpoint best\n",
    "            if val_rmse < best_rmse:\n",
    "                best_rmse = val_rmse\n",
    "                best_state = stack_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Linear Stacking RMSE: 0.8592\n"
     ]
    }
   ],
   "source": [
    "stack_model.load_state_dict(best_state)\n",
    "stack_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_preds = stack_model(X_valid_tensor).clamp(1, 5).cpu().numpy()\n",
    "    final_rmse = np.sqrt(np.mean((final_preds - true_ratings_valid) ** 2))\n",
    "\n",
    "print(f\"\\nFinal Linear Stacking RMSE: {final_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rank the base models by their validation RMSE and select the top \\(k\\). For those \\(k\\) models, we assign weights proportional to the inverse RMSE:\n",
    "\n",
    "$$\n",
    "w_m = \\frac{1/\\mathrm{RMSE}_m}{\\sum_{j=1}^k (1/\\mathrm{RMSE}_j)},\n",
    "$$\n",
    "\n",
    "then compute the ensemble prediction\n",
    "\n",
    "$$\n",
    "\\hat r_i = \\sum_{m=1}^k w_m\\,\\hat r_i^{(m)}.\n",
    "$$\n",
    "\n",
    "By varying \\(k\\) from 2 up to the total number of models, we evaluate the RMSE of each top-\\(k\\) ensemble and choose the one with the lowest error.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ensemble(k=3):\n",
    "   \n",
    "    sorted_models = sorted(base_model_rmses.items(), key=lambda x: x[1])\n",
    "    top_models = sorted_models[:k]\n",
    "    top_model_names = [name for name, _ in top_models]\n",
    "\n",
    "    print(f\"Selected top {k} models: {', '.join(top_model_names)}\")\n",
    "\n",
    "\n",
    "    weights = {}\n",
    "    sum_weights = sum(1/ rmse for _, rmse in top_models)\n",
    "    for name, rmse in top_models:\n",
    "        weights[name] = (1 / rmse) / sum_weights\n",
    "        print(f\"  {name} weight: {weights[name]:.4f}\")\n",
    "\n",
    "    preds = np.zeros_like(true_ratings_valid, dtype=float)\n",
    "    for name in top_model_names:\n",
    "        preds += weights[name] * valid_predictions[name]\n",
    "\n",
    "    rmse = np.sqrt(np.mean((preds - true_ratings_valid) ** 2))\n",
    "    return preds, rmse, weights, top_model_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 2 models: DMF, NeuMF\n",
      "  DMF weight: 0.5003\n",
      "  NeuMF weight: 0.4997\n",
      "Selected top 3 models: DMF, NeuMF, GraphNeuMF\n",
      "  DMF weight: 0.3340\n",
      "  NeuMF weight: 0.3337\n",
      "  GraphNeuMF weight: 0.3323\n",
      "Selected top 4 models: DMF, NeuMF, GraphNeuMF, SVD++\n",
      "  DMF weight: 0.2526\n",
      "  NeuMF weight: 0.2523\n",
      "  GraphNeuMF weight: 0.2513\n",
      "  SVD++ weight: 0.2438\n",
      "Best Top-3 Ensemble RMSE: 0.835264\n"
     ]
    }
   ],
   "source": [
    "best_k = 0\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for k in range(2, len(prediction_functions) + 1):\n",
    "    _, rmse, _, _ = top_k_ensemble(k)\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Best Top-{best_k} Ensemble RMSE: {best_rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the optimal ensemble uses the top $k=3$ models with inverse‐RMSE weights, so we compute the blended predictions accordingly and call `make_submission(ensemble_pred_fn, \"ensemble_submission.csv\")` to generate the final submission.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 3 models: DMF, NeuMF, GraphNeuMF\n",
      "  DMF weight: 0.3340\n",
      "  NeuMF weight: 0.3337\n",
      "  GraphNeuMF weight: 0.3323\n"
     ]
    }
   ],
   "source": [
    "_, _, best_weights, best_names = top_k_ensemble(best_k)\n",
    "\n",
    "def ensemble_pred_fn(sids: np.ndarray, pids: np.ndarray) -> np.ndarray:\n",
    "    blended = np.zeros_like(sids, dtype=float)\n",
    "    for name in best_names:\n",
    "        blended += best_weights[name] * prediction_functions[name](sids, pids)\n",
    "    return blended\n",
    "\n",
    "make_submission(ensemble_pred_fn, \"ensemble_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
